#数据目录，约定读取挂载数据的目录
data_path = "/datacon2024/AI/CQ2"
model_path = "/datacon2024/AI/Models"

#具体的文件
q_file_path=f"{data_path}/Q.txt"
kb_file_path = f"{data_path}/KB.txt"
#输出答案的文件
output_file = "/result.csv"

import os
import csv
from FlagEmbedding import FlagModel
import numpy as np

# 加载中文或英文嵌入模型
def load_EM_M_zh():
    model = FlagModel(os.path.join(model_path, 'bge-large-zh-v1.5'),
                      query_instruction_for_retrieval="为这个句子生成表示以用于检索相关文章：",
                      use_fp16=True)
    return model


def load_EM_M_en():
    model = FlagModel(os.path.join(model_path, 'bge-large-en-v1.5'),
                      query_instruction_for_retrieval="For this sentence, generate a representation for retrieving related articles:",
                      use_fp16=True)
    return model

# 批量处理上下文检索
def batch_retrieve_context(queries, kb_sentences, model, top_k=3):
    queries_embeddings = model.encode(queries)
    kb_embeddings = model.encode(kb_sentences)

    contexts = []
    for query_embedding in queries_embeddings:
        similarities = np.dot(kb_embeddings, query_embedding)
        top_k_idx = np.argsort(similarities)[-top_k:]
        top_k_context = " ".join([kb_sentences[i] for i in top_k_idx])
        contexts.append(top_k_context)

    return contexts


# 构建优化后的提示
def create_prompt(query, context):
    prompt = (
        f"Question: {query}\n"
        f"Context: {context}\n"
        "Answer the question based on the context provided above. "
        "Do not include any information that is not in the context. "
        "Be precise and concise in your answer. "
        "Answer:"
    )
    return prompt

# RAG(·)
def Mitigate_hallucination(Q_path, KB_path, output_path='./result.csv'):
    """
    Mitigates hallucination in answers generated by a Large Language Model (LLM) 
    by retrieving relevant context from a Knowledge Base (KB file) for queries in a given Q file.
    
    This function uses Retrieval-Augmented Generation (RAG) and Prompt Engineering 
    techniques to produce low-hallucination, high-quality answers from the LLM.

    :param Q_path: The path to the Q file containing queries.
    :type Q_path: str

    :param KB_path: The path to the Knowledge Base file.
    :type KB_path: str

    :param output_path: The path where the resulting CSV file will be saved.
    :type output_path: str, optional

    :return: None
    """
    model = load_EM_M_en()

    try:
        with open(Q_path, 'r', encoding='utf-8') as input_file, \
                open(KB_path, 'r', encoding='utf-8') as kb_file, \
                open(output_path, 'w', newline='', encoding='utf-8') as output_file:

            kb_sentences = [line.strip() for line in kb_file.readlines()]
            writer = csv.writer(output_file)
            writer.writerow(['Query', 'Context', 'Prompt'])

            queries = [line.strip() for line in input_file.readlines()]
            contexts = batch_retrieve_context(queries, kb_sentences, model)

            for query, context in zip(queries, contexts):
                prompt = create_prompt(query, context)
                writer.writerow([query, context, prompt])

            print(f"Results have been successfully saved to {output_path}")

    except FileNotFoundError:
        print(f"The file {Q_path} or {KB_path} does not exist.")
    except IOError as e:
        print(f"An I/O error occurred: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    else:
        print(f"File {output_path} has been successfully generated.")


if __name__ == "__main__":
    Mitigate_hallucination(Q_path=q_file_path, KB_path=kb_file_path, output_path=output_file)
